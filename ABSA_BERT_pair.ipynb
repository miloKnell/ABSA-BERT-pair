{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ABSA-BERT-pair.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0s7ckJWgtOPERMnlUvlMg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miloKnell/ABSA-BERT-pair/blob/master/ABSA_BERT_pair.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIoEld2eaYmC",
        "colab_type": "code",
        "outputId": "c3485b17-e13d-4838-f494-9d016aecf906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "print(os.listdir())\n",
        "os.chdir('drive/My Drive')\n",
        "print(list(os.listdir('bert_files')))\n",
        "os.chdir('bert_files')\n",
        "#with open('vocab.txt') as f:\n",
        "##  c=f.readlines()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['bert_model.ckpt.data-00000-of-00001', 'pytorch_model.bin', 'bert_config.json', 'vocab.txt.gdoc', 'vocab.gdoc', 'vocab.txt', 'sentihood', 'data', 'output_data']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e3f8777a6ba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/My Drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert_files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert_files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jHoFEz0EDEm",
        "colab_type": "code",
        "outputId": "b90a8117-dd54-4be3-d6b7-54d4b0948d04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 30 05:43:51 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    29W /  70W |   8013MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D2-kykeXgFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myargparser():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def add_argument(self,name,default,type,choices=None,help=None,action=None):\n",
        "    cmd=\"self.{}=type('{}')\".format(name[2:],default)\n",
        "    exec(cmd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RelOGlXgS0pL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "\n",
        "def warmup_cosine(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 0.5 * (1.0 + torch.cos(math.pi * x))\n",
        "\n",
        "def warmup_constant(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 1.0\n",
        "\n",
        "def warmup_linear(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 1.0 - x\n",
        "\n",
        "SCHEDULES = {\n",
        "    'warmup_cosine':warmup_cosine,\n",
        "    'warmup_constant':warmup_constant,\n",
        "    'warmup_linear':warmup_linear,\n",
        "}\n",
        "\n",
        "\n",
        "class BERTAdam(Optimizer):\n",
        "    \"\"\"Implements BERT version of Adam algorithm with weight decay fix (and no ).\n",
        "    Params:\n",
        "        lr: learning rate\n",
        "        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n",
        "        t_total: total number of training steps for the learning\n",
        "            rate schedule, -1  means constant learning rate. Default: -1\n",
        "        schedule: schedule to use for the warmup (see above). Default: 'warmup_linear'\n",
        "        b1: Adams b1. Default: 0.9\n",
        "        b2: Adams b2. Default: 0.999\n",
        "        e: Adams epsilon. Default: 1e-6\n",
        "        weight_decay_rate: Weight decay. Default: 0.01\n",
        "        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr, warmup=-1, t_total=-1, schedule='warmup_linear',\n",
        "                 b1=0.9, b2=0.999, e=1e-6, weight_decay_rate=0.01,\n",
        "                 max_grad_norm=1.0):\n",
        "        if not lr >= 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
        "        if schedule not in SCHEDULES:\n",
        "            raise ValueError(\"Invalid schedule parameter: {}\".format(schedule))\n",
        "        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n",
        "            raise ValueError(\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\".format(warmup))\n",
        "        if not 0.0 <= b1 < 1.0:\n",
        "            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n",
        "        if not 0.0 <= b2 < 1.0:\n",
        "            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n",
        "        if not e >= 0.0:\n",
        "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(e))\n",
        "        defaults = dict(lr=lr, schedule=schedule, warmup=warmup, t_total=t_total,\n",
        "                        b1=b1, b2=b2, e=e, weight_decay_rate=weight_decay_rate,\n",
        "                        max_grad_norm=max_grad_norm)\n",
        "        super(BERTAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr = []\n",
        "        print(\"l_total=\",len(self.param_groups))\n",
        "        for group in self.param_groups:\n",
        "            print(\"l_p=\",len(group['params']))\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    return [0]\n",
        "                if group['t_total'] != -1:\n",
        "                    schedule_fct = SCHEDULES[group['schedule']]\n",
        "                    lr_scheduled = group['lr'] * schedule_fct(state['step']/group['t_total'], group['warmup'])\n",
        "                else:\n",
        "                    lr_scheduled = group['lr']\n",
        "                lr.append(lr_scheduled)\n",
        "        return lr\n",
        "\n",
        "    def to(self, device):\n",
        "        \"\"\" Move the optimizer state to a specified device\"\"\"\n",
        "        for state in self.state.values():\n",
        "            state['exp_avg'].to(device)\n",
        "            state['exp_avg_sq'].to(device)\n",
        "\n",
        "    def initialize_step(self, initial_step):\n",
        "        \"\"\"Initialize state with a defined step (but we don't have stored averaged).\n",
        "        Arguments:\n",
        "            initial_step (int): Initial step number.\n",
        "        \"\"\"\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                # State initialization\n",
        "                state['step'] = initial_step\n",
        "                # Exponential moving average of gradient values\n",
        "                state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                # Exponential moving average of squared gradient values\n",
        "                state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['next_m'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['next_v'] = torch.zeros_like(p.data)\n",
        "\n",
        "                next_m, next_v = state['next_m'], state['next_v']\n",
        "                beta1, beta2 = group['b1'], group['b2']\n",
        "\n",
        "                # Add grad clipping\n",
        "                if group['max_grad_norm'] > 0:\n",
        "                    clip_grad_norm_(p, group['max_grad_norm'])\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # In-place operations to update the averages at the same time\n",
        "                next_m.mul_(beta1).add_(1 - beta1, grad)\n",
        "                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                update = next_m / (next_v.sqrt() + group['e'])\n",
        "\n",
        "                # Just adding the square of the weights to the loss function is *not*\n",
        "                # the correct way of using L2 regularization/weight decay with Adam,\n",
        "                # since that will interact with the m and v parameters in strange ways.\n",
        "                #\n",
        "                # Instead we want ot decay the weights in a manner that doesn't interact\n",
        "                # with the m/v parameters. This is equivalent to adding the square\n",
        "                # of the weights to the loss with plain (non-momentum) SGD.\n",
        "                if group['weight_decay_rate'] > 0.0:\n",
        "                    update += group['weight_decay_rate'] * p.data\n",
        "\n",
        "                if group['t_total'] != -1:\n",
        "                    schedule_fct = SCHEDULES[group['schedule']]\n",
        "                    lr_scheduled = group['lr'] * schedule_fct(state['step']/group['t_total'], group['warmup'])\n",
        "                else:\n",
        "                    lr_scheduled = group['lr']\n",
        "\n",
        "                update_with_lr = lr_scheduled * update\n",
        "                p.data.add_(-update_with_lr)\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n",
        "                # bias_correction1 = 1 - beta1 ** state['step']\n",
        "                # bias_correction2 = 1 - beta2 ** state['step']\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zjRA-IxTI16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import collections\n",
        "import unicodedata\n",
        "\n",
        "import six\n",
        "\n",
        "\n",
        "def convert_to_unicode(text):\n",
        "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
        "    if six.PY3:\n",
        "        if isinstance(text, str):\n",
        "            return text\n",
        "        elif isinstance(text, bytes):\n",
        "            return text.decode(\"utf-8\", \"ignore\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    elif six.PY2:\n",
        "        if isinstance(text, str):\n",
        "            return text.decode(\"utf-8\", \"ignore\")\n",
        "        elif isinstance(text, unicode):\n",
        "            return text\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    else:\n",
        "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def printable_text(text):\n",
        "    \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
        "\n",
        "    # These functions want `str` for both Python2 and Python3, but in one case\n",
        "    # it's a Unicode string and in the other it's a byte string.\n",
        "    if six.PY3:\n",
        "        if isinstance(text, str):\n",
        "            return text\n",
        "        elif isinstance(text, bytes):\n",
        "            return text.decode(\"utf-8\", \"ignore\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    elif six.PY2:\n",
        "        if isinstance(text, str):\n",
        "            return text\n",
        "        elif isinstance(text, unicode):\n",
        "            return text.encode(\"utf-8\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    else:\n",
        "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "    vocab = collections.OrderedDict()\n",
        "    index = 0\n",
        "    with open(vocab_file, \"r\",encoding='utf-8') as reader:\n",
        "        while True:\n",
        "            token = convert_to_unicode(reader.readline())\n",
        "            if not token:\n",
        "                break\n",
        "            token = token.strip()\n",
        "            vocab[token] = index\n",
        "            index += 1\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def convert_tokens_to_ids(vocab, tokens):\n",
        "    \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
        "    ids = []\n",
        "    for token in tokens:\n",
        "        ids.append(vocab[token])\n",
        "    return ids\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "    \"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "\n",
        "class FullTokenizer(object):\n",
        "    \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_file, do_lower_case=True):\n",
        "        self.vocab = load_vocab(vocab_file)\n",
        "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        split_tokens = []\n",
        "        for token in self.basic_tokenizer.tokenize(text):\n",
        "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "                split_tokens.append(sub_token)\n",
        "\n",
        "        return split_tokens\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        return convert_tokens_to_ids(self.vocab, tokens)\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "    def __init__(self, do_lower_case=True):\n",
        "        \"\"\"Constructs a BasicTokenizer.\n",
        "\n",
        "        Args:\n",
        "          do_lower_case: Whether to lower case the input.\n",
        "        \"\"\"\n",
        "        self.do_lower_case = do_lower_case\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text.\"\"\"\n",
        "        text = convert_to_unicode(text)\n",
        "        text = self._clean_text(text)\n",
        "        orig_tokens = whitespace_tokenize(text)\n",
        "        split_tokens = []\n",
        "        for token in orig_tokens:\n",
        "            if self.do_lower_case:\n",
        "                token = token.lower()\n",
        "                token = self._run_strip_accents(token)\n",
        "            split_tokens.extend(self._run_split_on_punc(token))\n",
        "\n",
        "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "        return output_tokens\n",
        "\n",
        "    def _run_strip_accents(self, text):\n",
        "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "        text = unicodedata.normalize(\"NFD\", text)\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cat = unicodedata.category(char)\n",
        "            if cat == \"Mn\":\n",
        "                continue\n",
        "            output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _run_split_on_punc(self, text):\n",
        "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "        chars = list(text)\n",
        "        i = 0\n",
        "        start_new_word = True\n",
        "        output = []\n",
        "        while i < len(chars):\n",
        "            char = chars[i]\n",
        "            if _is_punctuation(char):\n",
        "                output.append([char])\n",
        "                start_new_word = True\n",
        "            else:\n",
        "                if start_new_word:\n",
        "                    output.append([])\n",
        "                start_new_word = False\n",
        "                output[-1].append(char)\n",
        "            i += 1\n",
        "\n",
        "        return [\"\".join(x) for x in output]\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "                continue\n",
        "            if _is_whitespace(char):\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n",
        "        self.vocab = vocab\n",
        "        self.unk_token = unk_token\n",
        "        self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "\n",
        "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "        using the given vocabulary.\n",
        "\n",
        "        For example:\n",
        "          input = \"unaffable\"\n",
        "          output = [\"un\", \"##aff\", \"##able\"]\n",
        "\n",
        "        Args:\n",
        "          text: A single token or whitespace separated tokens. This should have\n",
        "            already been passed through `BasicTokenizer.\n",
        "\n",
        "        Returns:\n",
        "          A list of wordpiece tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        text = convert_to_unicode(text)\n",
        "\n",
        "        output_tokens = []\n",
        "        for token in whitespace_tokenize(text):\n",
        "            chars = list(token)\n",
        "            if len(chars) > self.max_input_chars_per_word:\n",
        "                output_tokens.append(self.unk_token)\n",
        "                continue\n",
        "\n",
        "            is_bad = False\n",
        "            start = 0\n",
        "            sub_tokens = []\n",
        "            while start < len(chars):\n",
        "                end = len(chars)\n",
        "                cur_substr = None\n",
        "                while start < end:\n",
        "                    substr = \"\".join(chars[start:end])\n",
        "                    if start > 0:\n",
        "                        substr = \"##\" + substr\n",
        "                    if substr in self.vocab:\n",
        "                        cur_substr = substr\n",
        "                        break\n",
        "                    end -= 1\n",
        "                if cur_substr is None:\n",
        "                    is_bad = True\n",
        "                    break\n",
        "                sub_tokens.append(cur_substr)\n",
        "                start = end\n",
        "\n",
        "            if is_bad:\n",
        "                output_tokens.append(self.unk_token)\n",
        "            else:\n",
        "                output_tokens.extend(sub_tokens)\n",
        "        return output_tokens\n",
        "\n",
        "\n",
        "def _is_whitespace(char):\n",
        "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "    # as whitespace since they are generally considered as such.\n",
        "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat == \"Zs\":\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "    # These are technically control characters but we count them as whitespace\n",
        "    # characters.\n",
        "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return False\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"C\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "    cp = ord(char)\n",
        "    # We treat all non-letter/number ASCII as punctuation.\n",
        "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "    # Punctuation class but we treat them as punctuation anyways, for\n",
        "    # consistency.\n",
        "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"P\"):\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTvbAV4dTVGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "\n",
        "# Reference: https://github.com/huggingface/pytorch-pretrained-BERT\n",
        "\n",
        "\"\"\"PyTorch BERT model.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "\n",
        "import six\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class BertConfig(object):\n",
        "    \"\"\"Configuration class to store the configuration of a `BertModel`.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                vocab_size,\n",
        "                hidden_size=768,\n",
        "                num_hidden_layers=12,\n",
        "                num_attention_heads=12,\n",
        "                intermediate_size=3072,\n",
        "                hidden_act=\"gelu\",\n",
        "                hidden_dropout_prob=0.1,\n",
        "                attention_probs_dropout_prob=0.1,\n",
        "                max_position_embeddings=512,\n",
        "                type_vocab_size=16,\n",
        "                initializer_range=0.02):\n",
        "        \"\"\"Constructs BertConfig.\n",
        "\n",
        "        Args:\n",
        "            vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n",
        "            hidden_size: Size of the encoder layers and the pooler layer.\n",
        "            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
        "            num_attention_heads: Number of attention heads for each attention layer in\n",
        "                the Transformer encoder.\n",
        "            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
        "                layer in the Transformer encoder.\n",
        "            hidden_act: The non-linear activation function (function or string) in the\n",
        "                encoder and pooler.\n",
        "            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
        "                layers in the embeddings, encoder, and pooler.\n",
        "            attention_probs_dropout_prob: The dropout ratio for the attention\n",
        "                probabilities.\n",
        "            max_position_embeddings: The maximum sequence length that this model might\n",
        "                ever be used with. Typically set this to something large just in case\n",
        "                (e.g., 512 or 1024 or 2048).\n",
        "            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
        "                `BertModel`.\n",
        "            initializer_range: The sttdev of the truncated_normal_initializer for\n",
        "                initializing all weight matrices.\n",
        "        \"\"\"\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.hidden_act = hidden_act\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.type_vocab_size = type_vocab_size\n",
        "        self.initializer_range = initializer_range\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, json_object):\n",
        "        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
        "        config = BertConfig(vocab_size=None)\n",
        "        for (key, value) in six.iteritems(json_object):\n",
        "            config.__dict__[key] = value\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_json_file(cls, json_file):\n",
        "        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "        with open(json_file, \"r\") as reader:\n",
        "            text = reader.read()\n",
        "        return cls.from_dict(json.loads(text))\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class BERTLayerNorm(nn.Module):\n",
        "    def __init__(self, config, variance_epsilon=1e-12):\n",
        "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "        \"\"\"\n",
        "        super(BERTLayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(config.hidden_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(config.hidden_size))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.gamma * x + self.beta\n",
        "\n",
        "class BERTEmbeddings(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTEmbeddings, self).__init__()\n",
        "        \"\"\"Construct the embedding module from word, position and token_type embeddings.\n",
        "        \"\"\"\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = BERTLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class BERTSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer\n",
        "\n",
        "\n",
        "class BERTSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BERTLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BERTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTAttention, self).__init__()\n",
        "        self.self = BERTSelfAttention(config)\n",
        "        self.output = BERTSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        self_output = self.self(input_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output\n",
        "\n",
        "\n",
        "class BERTIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTIntermediate, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = gelu\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BERTOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BERTLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BERTLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTLayer, self).__init__()\n",
        "        self.attention = BERTAttention(config)\n",
        "        self.intermediate = BERTIntermediate(config)\n",
        "        self.output = BERTOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        attention_output = self.attention(hidden_states, attention_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "class BERTEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTEncoder, self).__init__()\n",
        "        layer = BERTLayer(config)\n",
        "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])    \n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        all_encoder_layers = []\n",
        "        for layer_module in self.layer:\n",
        "            hidden_states = layer_module(hidden_states, attention_mask)\n",
        "            all_encoder_layers.append(hidden_states)\n",
        "        return all_encoder_layers\n",
        "\n",
        "\n",
        "class BERTPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTPooler, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        #return first_token_tensor\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class BertModel(nn.Module):\n",
        "    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "    config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "    model = modeling.BertModel(config=config)\n",
        "    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config: BertConfig):\n",
        "        \"\"\"Constructor for BertModel.\n",
        "\n",
        "        Args:\n",
        "            config: `BertConfig` instance.\n",
        "        \"\"\"\n",
        "        super(BertModel, self).__init__()\n",
        "        self.embeddings = BERTEmbeddings(config)\n",
        "        self.encoder = BERTEncoder(config)\n",
        "        self.pooler = BERTPooler(config)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, from_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, to_seq_length, from_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.float()\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
        "        all_encoder_layers = self.encoder(embedding_output, extended_attention_mask)\n",
        "        sequence_output = all_encoder_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        return all_encoder_layers, pooled_output\n",
        "\n",
        "class BertForSequenceClassification(nn.Module):\n",
        "    \"\"\"BERT model for classification.\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the pooled output.\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "    config = BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "    num_labels = 2\n",
        "\n",
        "    model = BertForSequenceClassification(config, num_labels)\n",
        "    logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, num_labels):\n",
        "        super(BertForSequenceClassification, self).__init__()\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "\n",
        "        def init_weights(module):\n",
        "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "                # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "                # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "            elif isinstance(module, BERTLayerNorm):\n",
        "                module.beta.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "                module.gamma.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "            if isinstance(module, nn.Linear):\n",
        "                module.bias.data.zero_()\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attention_mask, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "            return loss, logits\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "\n",
        "class BertForQuestionAnswering(nn.Module):\n",
        "    \"\"\"BERT model for Question Answering (span extraction).\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the sequence output that computes start_logits and end_logits\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "    config = BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "    model = BertForQuestionAnswering(config)\n",
        "    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForQuestionAnswering, self).__init__()\n",
        "        self.bert = BertModel(config)\n",
        "        # TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version\n",
        "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "        def init_weights(module):\n",
        "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "                # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "                # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "            elif isinstance(module, BERTLayerNorm):\n",
        "                module.beta.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "                module.gamma.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "            if isinstance(module, nn.Linear):\n",
        "                module.bias.data.zero_()\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attention_mask, start_positions=None, end_positions=None):\n",
        "        all_encoder_layers, _ = self.bert(input_ids, token_type_ids, attention_mask)\n",
        "        sequence_output = all_encoder_layers[-1]\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension - if not this is a no-op\n",
        "            start_positions = start_positions.squeeze(-1)\n",
        "            end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions.clamp_(0, ignored_index)\n",
        "            end_positions.clamp_(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "            return total_loss\n",
        "        else:\n",
        "            return start_logits, end_logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O505xXcWTahD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "\n",
        "\"\"\"Processors for different tasks.\"\"\"\n",
        "\n",
        "import csv\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "\n",
        "class Sentihood_single_Processor(DataProcessor):\n",
        "    \"\"\"Processor for the Sentihood data set.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        train_data = pd.read_csv(os.path.join(data_dir, \"train.tsv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(train_data, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        dev_data = pd.read_csv(os.path.join(data_dir, \"dev.tsv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(dev_data, \"dev\")\n",
        "    \n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        test_data = pd.read_csv(os.path.join(data_dir, \"test.tsv\"),header=None,sep=\"\\t\").values\n",
        "        return self._create_examples(test_data, \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return ['None', 'Positive', 'Negative']\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "          #  if i>50:break\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = convert_to_unicode(str(line[1]))\n",
        "            label = convert_to_unicode(str(line[2]))\n",
        "            if i%1000==0:\n",
        "                print(i)\n",
        "                print(\"guid=\",guid)\n",
        "                print(\"text_a=\",text_a)\n",
        "                print(\"label=\",label)\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "        \n",
        "class Sentihood_QA_M_Processor(DataProcessor):\n",
        "    \"\"\"Processor for the Sentihood data set.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        train_data = pd.read_csv(os.path.join(data_dir, \"train_QA_M.tsv\"),sep=\"\\t\").values\n",
        "        return self._create_examples(train_data, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        dev_data = pd.read_csv(os.path.join(data_dir, \"dev_QA_M.tsv\"),sep=\"\\t\").values\n",
        "        return self._create_examples(dev_data, \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        test_data = pd.read_csv(os.path.join(data_dir, \"test_QA_M.tsv\"),sep=\"\\t\").values\n",
        "        return self._create_examples(test_data, \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return ['None', 'Positive', 'Negative']\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i%4==0:\n",
        "                guid = \"%s-%s\" % (set_type, i)\n",
        "                text_a = convert_to_unicode(str(line[1]))\n",
        "                text_b = convert_to_unicode(str(line[2]))\n",
        "                label = convert_to_unicode(str(line[3]))\n",
        "                '''if i%1000==0:\n",
        "                    print(i)\n",
        "                    print(\"guid=\",guid)\n",
        "                    print(\"text_a=\",text_a)\n",
        "                    print(\"text_b=\",text_b)\n",
        "                    print(\"label=\",label)'''\n",
        "                if label is not None: #ADDED\n",
        "                    examples.append(\n",
        "                        InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "        print(len(examples))\n",
        "        return examples\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFgAI0BAT29k",
        "colab_type": "code",
        "outputId": "dc85ea5e-ba68-4b4b-80a9-fbea7caa1117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import argparse\n",
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    label_map = {}\n",
        "    for (i, label) in enumerate(label_list):\n",
        "        label_map[label] = i\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "\n",
        "        if tokens_b:\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids: 0   0   0   0  0     0 0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens = []\n",
        "        segment_ids = []\n",
        "        tokens.append(\"[CLS]\")\n",
        "        segment_ids.append(0)\n",
        "        for token in tokens_a:\n",
        "            tokens.append(token)\n",
        "            segment_ids.append(0)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(0)\n",
        "\n",
        "        if tokens_b:\n",
        "            for token in tokens_b:\n",
        "                tokens.append(token)\n",
        "                segment_ids.append(1)\n",
        "            tokens.append(\"[SEP]\")\n",
        "            segment_ids.append(1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        while len(input_ids) < max_seq_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "            segment_ids.append(0)\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        label_id = label_map[example.label]\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(\n",
        "                        input_ids=input_ids,\n",
        "                        input_mask=input_mask,\n",
        "                        segment_ids=segment_ids,\n",
        "                        label_id=label_id))\n",
        "    return features\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#parser = ArgumentParser()\n",
        "parser=myargparser()\n",
        "\n",
        "## Required parameters\n",
        "parser.add_argument(name=\"--task_name\",\n",
        "                    default='sentihood_QA_M', #CHANGED\n",
        "                    type=str,\n",
        "                    help=\"The name of the task to train.\")\n",
        "parser.add_argument(name=\"--data_dir\",\n",
        "                    default='sentihood/bert-pair', #CHANGED\n",
        "                    type=str,\n",
        "                    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
        "parser.add_argument(name=\"--vocab_file\",\n",
        "                    default='vocab.txt', #CHANGED\n",
        "                    type=str,\n",
        "                    help=\"The vocabulary file that the BERT model was trained on.\")\n",
        "parser.add_argument(\"--bert_config_file\",\n",
        "                    default='bert_config.json',#CHANGED\n",
        "                    type=str,\n",
        "                    help=\"The config json file corresponding to the pre-trained BERT model. \\n\"\n",
        "                         \"This specifies the model architecture.\")\n",
        "parser.add_argument(\"--output_dir\",\n",
        "                    default='sentihood/QA_M',#CHANGED\n",
        "                    type=str,\n",
        "                    help=\"The output directory where the model checkpoints will be written.\")\n",
        "parser.add_argument(\"--init_checkpoint\",\n",
        "                    default='pytorch_model.bin', #CHANGED\n",
        "                    type=str,\n",
        "                    help=\"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
        "\n",
        "## Other parameters\n",
        "parser.add_argument(\"--eval_test\",\n",
        "                    default=True, #CHANGED\n",
        "                    type=bool,\n",
        "                    action='store_true',\n",
        "                    help=\"Whether to run eval on the test set.\")                    \n",
        "parser.add_argument(\"--do_lower_case\",\n",
        "                    default=True, #CHANGED\n",
        "                    type=bool,\n",
        "                    action='store_true',\n",
        "                    help=\"Whether to lower case the input text. True for uncased models, False for cased models.\")\n",
        "parser.add_argument(\"--max_seq_length\",\n",
        "                    default=128, #CHANGED\n",
        "                    type=int,\n",
        "                    help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
        "                         \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
        "                         \"than this will be padded.\")\n",
        "parser.add_argument(\"--train_batch_size\",\n",
        "                    default=32, #CHANGED\n",
        "                    type=int,\n",
        "                    help=\"Total batch size for training.\")\n",
        "parser.add_argument(\"--eval_batch_size\",\n",
        "                    default=8,\n",
        "                    type=int,\n",
        "                    help=\"Total batch size for eval.\")\n",
        "parser.add_argument(\"--learning_rate\",\n",
        "                    default=2e-5, #CHANGED\n",
        "                    type=float,\n",
        "                    help=\"The initial learning rate for Adam.\")\n",
        "parser.add_argument(\"--num_train_epochs\",\n",
        "                    default=6.0, #CHANGED\n",
        "                    type=float,\n",
        "                    help=\"Total number of training epochs to perform.\")\n",
        "parser.add_argument(\"--warmup_proportion\",\n",
        "                    default=0.1,\n",
        "                    type=float,\n",
        "                    help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
        "                         \"E.g., 0.1 = 10%% of training.\")\n",
        "parser.add_argument(\"--no_cuda\",\n",
        "                    default=False,\n",
        "                    action='store_true',\n",
        "                    type=bool,\n",
        "                    help=\"Whether not to use CUDA when available\")\n",
        "parser.add_argument(\"--accumulate_gradients\",\n",
        "                    type=int,\n",
        "                    default=1,\n",
        "                    help=\"Number of steps to accumulate gradient on (divide the batch_size and accumulate)\")\n",
        "parser.add_argument(\"--local_rank\",\n",
        "                    type=int,\n",
        "                    default=-1,\n",
        "                    help=\"local_rank for distributed training on gpus\")\n",
        "parser.add_argument('--seed', \n",
        "                    type=int, \n",
        "                    default=42,\n",
        "                    help=\"random seed for initialization\")\n",
        "parser.add_argument('--gradient_accumulation_steps',\n",
        "                    type=int,\n",
        "                    default=1,\n",
        "                    help=\"Number of updates steps to accumualte before performing a backward/update pass.\")\n",
        "#sys.argv = '[-f]' \n",
        "args=parser                  \n",
        "#args = parser.parse_args()\n",
        "#os.chdir('bert_files')\n",
        "\n",
        "if args.local_rank == -1 or args.no_cuda:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "else:\n",
        "    device = torch.device(\"cuda\", args.local_rank)\n",
        "    n_gpu = 1\n",
        "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.distributed.init_process_group(backend='nccl')\n",
        "#logger.info(\"device %s n_gpu %d distributed training %r\", device, n_gpu, bool(args.local_rank != -1))\n",
        "\n",
        "if args.accumulate_gradients < 1:\n",
        "    raise ValueError(\"Invalid accumulate_gradients parameter: {}, should be >= 1\".format(\n",
        "                        args.accumulate_gradients))\n",
        "\n",
        "args.train_batch_size = int(args.train_batch_size / args.accumulate_gradients)\n",
        "\n",
        "bert_config = BertConfig.from_json_file(args.bert_config_file)\n",
        "\n",
        "if args.max_seq_length > bert_config.max_position_embeddings:\n",
        "    raise ValueError(\n",
        "        \"Cannot use sequence length {} because the BERT model was only trained up to sequence length {}\".format(\n",
        "        args.max_seq_length, bert_config.max_position_embeddings))\n",
        "\n",
        "if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n",
        "    raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
        "os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# prepare dataloaders\n",
        "\n",
        "processor = Sentihood_QA_M_Processor()\n",
        "label_list = processor.get_labels()\n",
        "\n",
        "tokenizer = FullTokenizer(\n",
        "    vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n",
        "\n",
        "# training set\n",
        "train_examples = None\n",
        "num_train_steps = None\n",
        "train_examples = processor.get_train_examples(args.data_dir)\n",
        "num_train_steps = int(\n",
        "    len(train_examples) / args.train_batch_size * args.num_train_epochs)\n",
        "\n",
        "train_features = convert_examples_to_features(\n",
        "    train_examples, label_list, args.max_seq_length, tokenizer)\n",
        "\n",
        "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
        "\n",
        "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "if args.local_rank == -1:\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "else:\n",
        "    train_sampler = DistributedSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "# test set\n",
        "if args.eval_test:\n",
        "    test_examples = processor.get_test_examples(args.data_dir)\n",
        "    test_features = convert_examples_to_features(\n",
        "        test_examples, label_list, args.max_seq_length, tokenizer)\n",
        "\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
        "    all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
        "\n",
        "    test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=args.eval_batch_size, shuffle=False)\n",
        "\n",
        "print('Optimizer set up')\n",
        "# model and optimizer\n",
        "model = BertForSequenceClassification(bert_config, len(label_list))\n",
        "if args.init_checkpoint is not None:\n",
        "    model.bert.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))\n",
        "model.to(device)\n",
        "\n",
        "if args.local_rank != -1:\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                      output_device=args.local_rank)\n",
        "elif n_gpu > 1:\n",
        "    model = torch.nn.DataParallel(model)\n",
        "\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_parameters = [\n",
        "     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
        "     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
        "     ]\n",
        "            \n",
        "optimizer = BERTAdam(optimizer_parameters,\n",
        "                     lr=args.learning_rate,\n",
        "                     warmup=args.warmup_proportion,\n",
        "                     t_total=num_train_steps)\n",
        "\n",
        "print('Training')\n",
        "# train\n",
        "output_data=[]\n",
        "f_test=[]\n",
        "global_step = 0\n",
        "epoch=0\n",
        "for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
        "    epoch+=1\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_steps = 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, input_mask, segment_ids, label_ids = batch\n",
        "        loss, _ = model(input_ids, segment_ids, input_mask, label_ids)\n",
        "        if n_gpu > 1:\n",
        "            loss = loss.mean() # mean() to average on multi-gpu.\n",
        "        if args.gradient_accumulation_steps > 1:\n",
        "            loss = loss / args.gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_steps += 1\n",
        "        if nb_tr_steps % 10 == 0:\n",
        "            print(nb_tr_steps)\n",
        "        optimizer.step() \n",
        "        model.zero_grad()\n",
        "        global_step += 1\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    test_loss, test_accuracy = 0, 0\n",
        "    nb_test_steps,nb_test_examples= 0,0\n",
        "    with open(os.path.join(args.output_dir, \"test_ep_\"+str(epoch)+\".txt\"),\"w\") as f_test:\n",
        "        for input_ids, input_mask, segment_ids, label_ids in test_dataloader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            input_mask = input_mask.to(device)\n",
        "            segment_ids = segment_ids.to(device)\n",
        "            label_ids = label_ids.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                tmp_test_loss, logits = model(input_ids, segment_ids, input_mask, label_ids)\n",
        "\n",
        "            logits = F.softmax(logits, dim=-1)\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = label_ids.to('cpu').numpy()\n",
        "            outputs = np.argmax(logits, axis=1)\n",
        "            for output_i in range(len(outputs)):\n",
        "                f_test.write(str(outputs[output_i]))\n",
        "                for ou in logits[output_i]:\n",
        "                    f_test.write(\" \"+str(ou))\n",
        "                f_test.write(\"\\n\")\n",
        "            tmp_test_accuracy=np.sum(outputs == label_ids)\n",
        "\n",
        "            test_loss += tmp_test_loss.mean().item()\n",
        "            test_accuracy += tmp_test_accuracy\n",
        "\n",
        "            nb_test_examples += input_ids.size(0)\n",
        "            nb_test_steps += 1\n",
        "\n",
        "    test_loss = test_loss / nb_test_steps\n",
        "    test_accuracy = test_accuracy / nb_test_examples\n",
        "    print(test_loss,test_accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "3752\n",
            "1879\n",
            "Optimizer set up\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/6 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  17%|█▋        | 1/6 [01:53<09:28, 113.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.3762237624919161 0.8467269824374667\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  33%|███▎      | 2/6 [03:46<07:34, 113.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.3790006899136178 0.8658861096327833\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 3/6 [05:40<05:40, 113.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.39778244872676566 0.854709952102182\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  67%|██████▋   | 4/6 [07:33<03:46, 113.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.41578010076538047 0.8690792974986695\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  83%|████████▎ | 5/6 [09:26<01:53, 113.31s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.426146707224085 0.8759978712080894\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 6/6 [11:19<00:00, 113.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.43705556969693365 0.8759978712080894\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}